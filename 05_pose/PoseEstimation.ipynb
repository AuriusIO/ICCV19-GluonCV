{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict pose using pretrained GluonCV models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from gluoncv import model_zoo, data, utils\n",
    "from gluoncv.data.transforms.pose import detector_to_simple_pose, heatmap_to_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a pretrained model\n",
    "-------------------------\n",
    "\n",
    "Let's get a Simple Pose model trained with input images of size 256x192 on MS COCO\n",
    "dataset. We pick the one using ResNet-18 V1b as the base model. By specifying\n",
    "``pretrained=True``, it will automatically download the model from the model\n",
    "zoo if necessary. For more pretrained models, please refer to\n",
    "https://gluon-cv.mxnet.io/model_zoo/pose.html.\n",
    "\n",
    "Note that a Simple Pose model takes a top-down strategy to estimate\n",
    "human pose in detected bounding boxes from an object detection model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = model_zoo.get_model('yolo3_mobilenet1.0_coco', pretrained=True)\n",
    "pose_net = model_zoo.get_model('simple_pose_resnet18_v1b', pretrained=True)\n",
    "\n",
    "# Note that we can reset the classes of the detector to only include\n",
    "# human, so that the NMS process is faster.\n",
    "\n",
    "detector.reset_class([\"person\"], reuse_weights=['person'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process an image for detector, and make inference\n",
    "--------------------\n",
    "\n",
    "Next we download an image, and pre-process with preset data transforms. Here we\n",
    "specify that we resize the short edge of the image to 512 px. But you can\n",
    "feed an arbitrarily sized image.\n",
    "\n",
    "This function returns two results. The first is a NDArray with shape\n",
    "``(batch_size, RGB_channels, height, width)``. It can be fed into the\n",
    "model directly. The second one contains the images in numpy format to\n",
    "easy to be plotted. Since we only loaded a single image, the first dimension\n",
    "of `x` is 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_fname = utils.download('https://github.com/dmlc/web-data/blob/master/' +\n",
    "                          'gluoncv/pose/soccer.png?raw=true',\n",
    "                          path='soccer.png')\n",
    "x, img = data.transforms.presets.ssd.load_test(im_fname, short=512)\n",
    "print('Shape of pre-processed image:', x.shape)\n",
    "\n",
    "class_IDs, scores, bounding_boxs = detector(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process tensor from detector to keypoint network\n",
    "--------------------\n",
    "\n",
    "Next we process the output from the detector.\n",
    "\n",
    "For a Simple Pose network, it expects the input has the size 256x192,\n",
    "and the human is centered. We crop the bounding boxed area\n",
    "for each human, and resize it to 256x192, then finally normalize it.\n",
    "\n",
    "In order to make sure the bounding box has included the entire person,\n",
    "we usually slightly upscale the box size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_input, upscale_bbox = detector_to_simple_pose(img, class_IDs, scores, bounding_boxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with a Simple Pose network\n",
    "--------------------\n",
    "\n",
    "Now we can make prediction.\n",
    "\n",
    "A Simple Pose network predicts the heatmap for each joint (i.e. keypoint).\n",
    "After the inference we search for the highest value in the heatmap and map it to the\n",
    "coordinates on the original image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_heatmap = pose_net(pose_input)\n",
    "pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the pose estimation results\n",
    "---------------------\n",
    "\n",
    "We can use `gluoncv.utils.viz.plot_keypoints` to visualize the\n",
    "results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = utils.viz.plot_keypoints(img, pred_coords, confidence,\n",
    "                              class_IDs, bounding_boxs, scores,\n",
    "                              box_thresh=0.5, keypoint_thresh=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dive deep into Training a Simple Pose Model on COCO Keypoints\n",
    "===================================================================\n",
    "\n",
    "In this tutorial, we show you how to train a pose estimation model [1]_ on the COCO dataset.\n",
    "\n",
    "First let's import some necessary modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import time, logging, os, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.data import mscoco\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, LRScheduler\n",
    "from gluoncv.data.transforms.presets.simple_pose import SimplePoseDefaultTrainTransform\n",
    "from gluoncv.utils.metrics import HeatmapAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data\n",
    "----------------\n",
    "\n",
    "We can load COCO Keypoints dataset with their official API\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mscoco.keypoints.COCOKeyPoints('~/.mxnet/datasets/coco',\n",
    "                                               splits=('person_keypoints_train2017'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset object enables us to retrieve images containing a person,\n",
    "the person's keypoints, and meta-information.\n",
    "\n",
    "Following the original paper, we resize the input to be ``(256, 192)``.\n",
    "For augmentation, we randomly scale, rotate or flip the input.\n",
    "Finally we normalize it with the standard ImageNet statistics.\n",
    "\n",
    "The COCO keypoints dataset contains 17 keypoints for a person.\n",
    "Each keypoint is annotated with three numbers ``(x, y, v)``, where ``x`` and ``y``\n",
    "mark the coordinates, and ``v`` indicates if the keypoint is visible.\n",
    "\n",
    "For each keypoint, we generate a gaussian kernel centered at the ``(x, y)`` coordinate, and use\n",
    "it as the training label. This means the model predicts a gaussian distribution on a feature map.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = SimplePoseDefaultTrainTransform(num_joints=train_dataset.num_joints,\n",
    "                                                  joint_pairs=train_dataset.joint_pairs,\n",
    "                                                  image_size=(256, 192), heatmap_size=(64, 48),\n",
    "                                                  scale_factor=0.30, rotation_factor=40, random_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our data loader with the dataset and transformation. We will iterate\n",
    "over ``train_data`` in our training loop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = gluon.data.DataLoader(\n",
    "    train_dataset.transform(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deconvolution Layer\n",
    "-------------------\n",
    "\n",
    "A deconvolution layer enlarges the feature map size of the input,\n",
    "so that it can be seen as a layer upsamling the input feature map.\n",
    "\n",
    "![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif)\n",
    "\n",
    "    :width: 40%\n",
    "    :align: center\n",
    "\n",
    "In the above image, the blue map is the input feature map, and the cyan map is the output.\n",
    "\n",
    "In a ``ResNet`` model, the last feature map shrinks its height and width to be only 1/32 of the input. It may\n",
    "be too small for a heatmap prediction. However if followed by several deconvolution layers, the feature map\n",
    "can have a larger size thus easier to make the prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition\n",
    "-----------------\n",
    "\n",
    "A Simple Pose model consists of a main body of a resnet, and several deconvolution layers.\n",
    "Its final layer is a convolution layer predicting one heatmap for each keypoint.\n",
    "\n",
    "Let's take a look at the smallest one from the GluonCV Model Zoo, using ``ResNet18`` as its base model.\n",
    "\n",
    "We load the pre-trained parameters for the ``ResNet18`` layers,\n",
    "and initialize the deconvolution layer and the final convolution layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.gpu(0)\n",
    "net = get_model('simple_pose_resnet18_v1b', num_joints=17, pretrained_base=True,\n",
    "                ctx=context, pretrained_ctx=context)\n",
    "net.deconv_layers.initialize(ctx=context)\n",
    "net.final_layer.initialize(ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the summary of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mx.nd.ones((1, 3, 256, 192), ctx=context)\n",
    "net.summary(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The Batch Normalization implementation from cuDNN has a negative impact on the model training,\n",
    "    as reported in these issues [2]_, [3]_ .\n",
    "\n",
    "    Since similar behavior is observed, we implement a ``BatchNormCudnnOff`` layer as a temporary solution.\n",
    "    This layer doesn't call the Batch Normalization layer from cuDNN, thus gives better results.</p></div>\n",
    "\n",
    "\n",
    "Training Setup\n",
    "--------------\n",
    "\n",
    "Next, we can set up everything for the training.\n",
    "\n",
    "- Loss:\n",
    "\n",
    "    We apply a weighted ``L2Loss`` on the predicted heatmap, where the weight is\n",
    "    1 if the keypoint is visible, otherwise is 0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning Rate Schedule and Optimizer:\n",
    "\n",
    "    We use an initial learning rate at 0.001, and divide it by 10 at the 90th and 120th epoch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_samples = len(train_dataset)\n",
    "num_batches = num_training_samples // batch_size\n",
    "lr_scheduler = LRScheduler(mode='step', base_lr=0.001,\n",
    "                           iters_per_epoch=num_batches, nepochs=140,\n",
    "                           step_epoch=(90, 120), step_factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we use ``adam`` as the optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'lr_scheduler': lr_scheduler})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metric\n",
    "\n",
    "    The metric for this model is called heatmap accuracy, i.e. it compares the\n",
    "    keypoint heatmaps from the prediction and groundtruth and check if the center\n",
    "    of the gaussian distributions are within a certain distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = HeatmapAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop\n",
    "-------------\n",
    "\n",
    "Since we have all necessary blocks, we can now put them together to start the training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hybridize(static_alloc=True, static_shape=True)\n",
    "for epoch in range(1):\n",
    "    metric.reset()\n",
    "\n",
    "    for i, batch in enumerate(train_data):\n",
    "        if i > 0:\n",
    "            break\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=[context], batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=[context], batch_axis=0)\n",
    "        weight = gluon.utils.split_and_load(batch[2], ctx_list=[context], batch_axis=0)\n",
    "\n",
    "        with ag.record():\n",
    "            outputs = [net(X) for X in data]\n",
    "            loss = [L(yhat, y, w) for yhat, y, w in zip(outputs, label, weight)]\n",
    "\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        metric.update(label, outputs)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limitation on the resources, we only train the model for one batch in this tutorial.\n",
    "\n",
    "Please checkout the full :download:`training script\n",
    "<../../../scripts/pose/simple_pose/train_simple_pose.py>` to reproduce our results.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Xiao, Bin, Haiping Wu, and Yichen Wei. \\\n",
    "       \"Simple baselines for human pose estimation and tracking.\" \\\n",
    "       Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n",
    ".. [2] https://github.com/Microsoft/human-pose-estimation.pytorch/issues/48\n",
    ".. [3] https://github.com/PaddlePaddle/models/tree/develop/fluid/PaddleCV/human_pose_estimation#known-issues\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
